is2 <- apriori(Adult, parameter = list(support= 0.1,  target = "rules"),
appearance = list(none = c("income=small", "income=large"),
default="both"))
is2 <- apriori(Adult, parameter = list(supp = 0.5, conf = 0.9, target = "rules"),
appearance = list(none = c("income=small", "income=large"),
is3 <- apriori(Adult, parameter = list(supp = 0.5, conf = 0.9, target = "rules"),
appearance = list(none = c("income=small", "income=large"),
default="both"))
is3
is3 <- apriori(Adult, parameter = list(supp = 0.5, conf = 0.9, target = "rules"),
appearance = list(none = c("income=small", "income=large"),
default="both"))
is3
summary(is3)
plot.igraph(ruleg, vertex.label=V(ruleg)$name,
plot.igraph(ruleg, vertex.label=V(ruleg)$name,
vertex.label.cex=1.2,vertex.label.color='black',
vertex.size=20, vertex.color='skyblue', vertex.frame.color='red')
plot.igraph(ruleg, vertex.label=V(ruleg)$name,
vertex.label.cex=1.2,vertex.label.color='black',
vertex.size=20, vertex.color='skyblue', vertex.frame.color='red')
plot.igraph(ruleg, vertex.label=V(ruleg)$name,
vertex.label.cex=1.2, vertex.label.color='black',
vertex.size=20, vertex.color='skyblue', vertex.frame.color='red')
plot.igraph(ruleg, vertex.label=V(ruleg)$name,
vertex.label.cex=1.2, vertex.label.color='black',
vertex.size=20, vertex.color='skyblue', vertex.frame.color='blue')
plot.igraph(ruleg, vertex.label=V(ruleg)$name,
vertex.label.cex=1.2, vertex.label.color='black',
vertex.size=20, vertex.color='green', vertex.frame.color='blue')
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre7')
library(rJava) # 아래와 같은 Error 발생 시 Sys.setenv()함수로 java 경로 지정
library(KoNLP) # rJava 라이브러리가 필요함
abstract2 <- file("C:/Rwork/Part-II/abstract2.txt", encoding="UTF-8")
abstract2l <- readLines(abstract2) # 줄 단위 데이터 생성
# incomplete final line found on - Error 발생 시 UTF-8 인코딩 방식으로 재 저장
close(abstract2)
head(abstract2l) # 앞부분 6줄 보기 - 줄 단위 문장 확인
lword <- Map(extractNoun, abstract2l)
length(lword) # [1] 472
lword <- unique(lword) # 중복제거1(전체 대상)
length(lword) # [1] 353(19개 제거)
lword <- sapply(lword, unique) # 중복제거2(줄 단위 대상)
length(lword) # [1] 352(1개 제거)
str(lword) # List of 353
lword # 추출 단어 확인
filter1 <- function(x){
nchar(x) <= 4 && nchar(x) >= 2 && is.hangul(x)
}
# 2) Filter(f,x) -> filter1() 함수를 적용하여 x 벡터 단위 필터링
filter2 <- function(x){
Filter(filter1, x)
}
# is.hangul() : KoNLP 패키지 제공
# Filter(f, x) : base
# nchar() : base -> 문자 수 반환
# 3) 줄 단어 대상 필터링
lword <- sapply(lword, filter2)
lword # 추출 단어 확인(길이 1개 단어 삭제됨)
tranrules <- apriori(wordtran, parameter=list(supp=0.25, conf=0.05))
inspect(tranrules) # 연관규칙 생성 결과 보기
rules <- labels(tranrules, ruleSep=" ")
rules <- sapply(rules, strsplit, " ",  USE.NAMES=F)
rulemat <- do.call("rbind", rules)
rulemat
library(igraph)
ruleg <- graph.edgelist(rulemat[c(22:59),],directed=F)
ruleg
plot.igraph(ruleg, vertex.label=V(ruleg)$name,
vertex.label.cex=1.2, vertex.label.color='black',
vertex.size=20, vertex.color='green', vertex.frame.color='blue')
abstract2 <- file("C:/Rwork/Part-II/abstract2.txt", encoding="UTF-8")
abstract2l <- readLines(abstract2) # 줄 단위 데이터 생성
# incomplete final line found on - Error 발생 시 UTF-8 인코딩 방식으로 재 저장
close(abstract2)
head(abstract2l) # 앞부분 6줄 보기 - 줄 단위 문장 확인
#----------------------------------------------------
# 2. 줄 단위 단어 추출
#----------------------------------------------------
# Map()함수 이용 줄 단위 단어 추출 : Map(f, ...) -> base)
lword <- Map(extractNoun, abstract2l)
length(lword) # [1] 472
lword <- unique(lword) # 중복제거1(전체 대상)
length(lword) # [1] 353(19개 제거)
lword <- sapply(lword, unique) # 중복제거2(줄 단위 대상)
length(lword) # [1] 352(1개 제거)
str(lword) # List of 353
lword # 추출 단어 확인
filter1 <- function(x){
nchar(x) <= 4 && nchar(x) >= 2 && is.hangul(x)
}
# 2) Filter(f,x) -> filter1() 함수를 적용하여 x 벡터 단위 필터링
filter2 <- function(x){
Filter(filter1, x)
}
# is.hangul() : KoNLP 패키지 제공
# Filter(f, x) : base
# nchar() : base -> 문자 수 반환
# 3) 줄 단어 대상 필터링
lword <- sapply(lword, filter2)
lword # 추출 단어 확인(길이 1개 단어 삭제됨)
word <- list(c("홍길동","이순","만기","김"),
c("대한민국","우리나라대한민국","한국","resu"))
class(word) # list
# (2) 단어 필터링 함수 정의 - 길이 2~4사이 한글 단어 추출
filter1 <- function(x){
nchar(x) <= 4 && nchar(x) >= 2 && is.hangul(x)
}
# Filter(f,x) -> f 함수를 적용하여 x 벡터 필터링
filter2 <- function(x){
Filter(filter1, x)
}
# (3) 함수를 이용하여 list 객체 대상 필터링
filterword <- sapply(word, filter2)
filterword
tranrules <- apriori(wordtran, parameter=list(supp=0.25, conf=0.05))
inspect(tranrules) # 연관규칙 생성 결과(59개) 보기
str(Adult)
dim(Adult) # [1] 48842   115
inspect(Adult) # 트랜잭션 데이터 보기
rules <- labels(tranrules, ruleSep=" ")
rules <- sapply(rules, strsplit, " ",  USE.NAMES=F)
rulemat <- do.call("rbind", rules)
rulemat
class(rulemat)
rules <- labels(tranrules, ruleSep=" ")
class(rules)
rules <- labels(tranrules, ruleSep=" ")
class(rules)
#[1] "character"
rules <- sapply(rules, strsplit, " ",  USE.NAMES=F)
rulemat <- do.call("rbind", rules)
# sapply(), do.call() # base 패키지
rulemat
class(rulemat)
#[1] "matrix"
library(igraph)
ruleg <- graph.edgelist(rulemat[c(22:59),], directed=F)
ruleg
rulemat
ruleg <- graph.edgelist(rulemat[c(12:59),], directed=F)
ruleg
ruleg
# (4) edgelist 시각화
plot.igraph(ruleg, vertex.label=V(ruleg)$name,
vertex.label.cex=1.2, vertex.label.color='black',
vertex.size=20, vertex.color='green', vertex.frame.color='blue')
plot.igraph(ruleg, vertex.label=V(ruleg)$name,
vertex.label.cex=1.2, vertex.label.color='black',
vertex.size=20, vertex.color='green', vertex.frame.color='blue')
plot.igraph(ruleg, vertex.label=V(ruleg)$name,
vertex.label.cex=1.2, vertex.label.color='black',
vertex.size=20, vertex.color='green', vertex.frame.color='blue')
# 6.단어 근접중심성(closeness centrality) 파악
closen <- closeness(ruleg)
#closen <- closen[-1] # {} 항목제거
closen
plot(closen, col="red",xaxt="n", lty="solid", type="b", xlab="단어", ylab="closeness")
points(closen, pch=16, col="navy")
axis(1, seq(1, length(closen)), V(ruleg)$name, cex=5)
# 중심성 : 노드(node)의 상대적 중요성을 나타내는 척도이다.
# plot, points(), axis() : graphics 패키지(기존 설치됨)
closen <- closeness(ruleg) # edgelist 대상 단어 근접중심성 생성
#closen <- closen[-1] # {} 항목제거
closen
closen <- <- closen[c(2:8)]
closen <- closen[c(2:8)]
plot(closen, col="red",xaxt="n", lty="solid", type="b", xlab="단어", ylab="closeness")
points(closen, pch=16, col="navy")
axis(1, seq(1, length(closen)), V(ruleg)$name, cex=5)
closen <- closen[-1]
plot(closen, col="red",xaxt="n", lty="solid", type="b", xlab="단어", ylab="closeness")
points(closen, pch=16, col="navy")
axis(1, seq(1, length(closen)), V(ruleg)$name, cex=5)
closen <- closeness(ruleg) # edgelist 대상 단어 근접중심성 생
#closen <- closen[c(2:8)] # {} 항목제거
closen <- closen[-1]
plot(closen, col="red",xaxt="n", lty="solid", type="b", xlab="단어", ylab="closeness")
points(closen, pch=16, col="navy")
axis(1, seq(1, length(closen)), V(ruleg)$name, cex=5)
axis(1, seq(1, length(closen)), V(ruleg)$name[-1], cex=5)
closen <- closeness(ruleg) # edgelist 대상 단어 근접중심성 생
closen <- closen[c(1:10)]
#closen <- closen[c(2:8)] # {} 항목제거
plot(closen, col="red",xaxt="n", lty="solid", type="b", xlab="단어", ylab="closeness")
points(closen, pch=16, col="navy")
axis(1, seq(1, length(closen)), V(ruleg)$name[c(1:10)], cex=5)
sentimental = function(sentences, posDic, negDic){
scores = laply(sentences, function(sentence, posDic, negDic) {
sentence = gsub('[[:punct:]]', '', sentence) #문장부호 제거
sentence = gsub('[[:cntrl:]]', '', sentence) #특수문자 제거
sentence = gsub('\\d+', '', sentence) # 숫자 제거
sentence = tolower(sentence) # 모두 소문자로 변경(단어가 모두 소문자 임)
word.list = str_split(sentence, '\\s+') # 공백 기준으로 단어 생성 -> \\s+ : 공백 정규식, +(1개 이상)
words = unlist(word.list) # unlist() : list를 vector 객체로 구조변경
pos.matches = match(words, posDic) # words의 단어를 posDic에서 matching
neg.matches = match(words, negDic)
pos.matches = !is.na(pos.matches) # NA 제거, 위치(숫자)만 추출
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches) # 긍정 - 부정
return(score)
}, posDic, negDic)
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
sentimental("I am victor and happy", posDic.final, negDic.final) # score = 1
sentimental = function(sentences, posDic, negDic){
scores = laply(sentences, function(sentence, posDic, negDic) {
sentence = gsub('[[:punct:]]', '', sentence) #문장부호 제거
sentence = gsub('[[:cntrl:]]', '', sentence) #특수문자 제거
sentence = gsub('\\d+', '', sentence) # 숫자 제거
sentence = tolower(sentence) # 모두 소문자로 변경(단어가 모두 소문자 임)
word.list = str_split(sentence, '\\s+') # 공백 기준으로 단어 생성 -> \\s+ : 공백 정규식, +(1개 이상)
words = unlist(word.list) # unlist() : list를 vector 객체로 구조변경
pos.matches = match(words, posDic) # words의 단어를 posDic에서 matching
neg.matches = match(words, negDic)
pos.matches = !is.na(pos.matches) # NA 제거, 위치(숫자)만 추출
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches) # 긍정 - 부정
return(score)
}, posDic, negDic)
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
sentimental("I am victor and happy", posDic.final, negDic.final) # score = 1
library(stringr) # str_split()함수 제공
sentimental = function(sentences, posDic, negDic){
scores = laply(sentences, function(sentence, posDic, negDic) {
sentence = gsub('[[:punct:]]', '', sentence) #문장부호 제거
sentence = gsub('[[:cntrl:]]', '', sentence) #특수문자 제거
sentence = gsub('\\d+', '', sentence) # 숫자 제거
sentence = tolower(sentence) # 모두 소문자로 변경(단어가 모두 소문자 임)
word.list = str_split(sentence, '\\s+') # 공백 기준으로 단어 생성 -> \\s+ : 공백 정규식, +(1개 이상)
words = unlist(word.list) # unlist() : list를 vector 객체로 구조변경
pos.matches = match(words, posDic) # words의 단어를 posDic에서 matching
neg.matches = match(words, negDic)
pos.matches = !is.na(pos.matches) # NA 제거, 위치(숫자)만 추출
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches) # 긍정 - 부정
return(score)
}, posDic, negDic)
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
sentimental("I am victor and happy", posDic.final, negDic.final) # score = 1
library(plyr) # laply()함수 제
sentimental = function(sentences, posDic, negDic){
scores = laply(sentences, function(sentence, posDic, negDic) {
sentence = gsub('[[:punct:]]', '', sentence) #문장부호 제거
sentence = gsub('[[:cntrl:]]', '', sentence) #특수문자 제거
sentence = gsub('\\d+', '', sentence) # 숫자 제거
sentence = tolower(sentence) # 모두 소문자로 변경(단어가 모두 소문자 임)
word.list = str_split(sentence, '\\s+') # 공백 기준으로 단어 생성 -> \\s+ : 공백 정규식, +(1개 이상)
words = unlist(word.list) # unlist() : list를 vector 객체로 구조변경
pos.matches = match(words, posDic) # words의 단어를 posDic에서 matching
neg.matches = match(words, negDic)
pos.matches = !is.na(pos.matches) # NA 제거, 위치(숫자)만 추출
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches) # 긍정 - 부정
return(score)
}, posDic, negDic)
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
sentimental("I am victor and happy", posDic.final, negDic.final) # score = 1
# (2) 긍정어/부정어 단어 추가
posDic.final <-c(posDic, 'victor')
negDic.final <-c(negDic, 'vanquished')
# 마지막에 단어 추가
posDic.final;
negDic.final
# 3) 감성 분석 함수 정의-score.sentiment
library(plyr) # laply()함수 제
library(stringr) # str_split()함수 제공
sentimental = function(sentences, posDic, negDic){
scores = laply(sentences, function(sentence, posDic, negDic) {
sentence = gsub('[[:punct:]]', '', sentence) #문장부호 제거
sentence = gsub('[[:cntrl:]]', '', sentence) #특수문자 제거
sentence = tolower(sentence) # 모두 소문자로 변경(단어가 모두 소문자 임)
word.list = str_split(sentence, '\\s+') # 공백 기준으로 단어 생성 -> \\s+ : 공백 정규식, +(1개 이상)
words = unlist(word.list) # unlist() : list를 vector 객체로 구조변경
neg.matches = match(words, negDic)
pos.matches = !is.na(pos.matches) # NA 제거, 위치(숫자)만 추출
neg.matches = !is.na(neg.matches)
return(score)
score = sum(pos.matches) - sum(neg.matches) # 긍정 - 부정
pos.matches = match(words, posDic) # words의 단어를 posDic에서 matching
sentence = gsub('\\d+', '', sentence) # 숫자 제거
}, posDic, negDic)
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
#[실습]--------------------------------------------------
# sentimental() 함수 기능 -> 긍정단어와 부정 단어 카운터 함수
# 긍정단어 테스트
sentimental("I am victor and happy", posDic.final, negDic.final) # score = 1
scores = laply(sentences, function(sentence, pos.words, neg.words) {
sentence = gsub('[[:punct:]]', '', sentence) #문장부호 제거
sentence = gsub('[[:cntrl:]]', '', sentence) #특수문자 제거
sentence = gsub('\\d+', '', sentence) # 숫자 제거
sentence = tolower(sentence) # 모두 소문자로 변경(단어가 모두 소문자 임)
word.list = str_split(sentence, '\\s+') # 공백 기준으로 단어 생성 -> \\s+ : 공백 정규식, +(1개 이상)
words = unlist(word.list) # unlist() : list를 vector 객체로 구조변경
pos.matches = match(words, pos.words) # words의 단어를 pos.words에서 match
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches) # NA 제거, 위치(숫자)만 추출
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches) # 긍정 - 부정
return(score)
}, pos.words, neg.words)
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
sentimental = function(sentences, pos.words, neg.words){
require(plyr)
require(stringr)
scores = laply(sentences, function(sentence, pos.words, neg.words) {
sentence = gsub('[[:cntrl:]]', '', sentence) #특수문자 제거
sentence = tolower(sentence) # 모두 소문자로 변경(단어가 모두 소문자 임)
word.list = str_split(sentence, '\\s+') # 공백 기준으로 단어 생성 -> \\s+ : 공백 정규식, +(1개 이상)
pos.matches = match(words, pos.words) # words의 단어를 pos.words에서 match
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches) # NA 제거, 위치(숫자)만 추출
score = sum(pos.matches) - sum(neg.matches) # 긍정 - 부정
return(score)
}, pos.words, neg.words)
return(scores.df)
}
scores.df = data.frame(score=scores, text=sentences)
sentence = gsub('\\d+', '', sentence) # 숫자 제거
sentence = gsub('[[:punct:]]', '', sentence) #문장부호 제거
words = unlist(word.list) # unlist() : list를 vector 객체로 구조변경
neg.matches = !is.na(neg.matches)
sentimental = function(sentences, posDic, negDic){
scores = laply(sentences, function(sentence, posDic, negDic) {
sentence = gsub('[[:punct:]]', '', sentence) #문장부호 제거
sentence = gsub('[[:cntrl:]]', '', sentence) #특수문자 제거
sentence = gsub('\\d+', '', sentence) # 숫자 제거
sentence = tolower(sentence) # 모두 소문자로 변경(단어가 모두 소문자 임)
word.list = str_split(sentence, '\\s+') # 공백 기준으로 단어 생성 -> \\s+ : 공백 정규식, +(1개 이상)
words = unlist(word.list) # unlist() : list를 vector 객체로 구조변경
pos.matches = match(words, posDic) # words의 단어를 posDic에서 matching
neg.matches = match(words, negDic)
pos.matches = !is.na(pos.matches) # NA 제거, 위치(숫자)만 추출
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches) # 긍정 - 부정
return(score)
}, posDic, negDic)
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
sentimental("I am victor and happy", posDic.final, negDic.final) # score = 1
posDic.final <-c(posDic, 'victor')
negDic.final <-c(negDic, 'vanquished')
# 마지막에 단어 추가
# 긍정어/부정어 영어 사전 가져오기
setwd("C:/Rwork/Part-II")
posDic <- readLines("posDic.txt")
negDic <- readLines("negDic.txt")
posDic.final <-c(posDic, 'victor')
negDic.final <-c(negDic, 'vanquished')
# 마지막에 단어 추가
posDic.final;
negDic.final
sentimental("I am victor and happy", posDic.final, negDic.final) # score = 1
# score       text
#1     1 I am Happy
# 부정단어 테스트
sentimental("I am vanquished unHappy", posDic.final, negDic.final) # score = -1
match(c("a","b","c"), c("b","c","d")) #  NA  1  2
match <- unlist(match)
matches <- !is.na(match)
match
matchResult <- unlist(match)
matchResult
matchResult <- !is.na(matchResult)
sum(as.numeric(matches))
setwd("C:/Rwork/Part-II")
posDic <- readLines("posDic.txt")
negDic <- readLines("negDic.txt")
length(posDic) # 2006
length(negDic) # 4783
# 사전에 단어추가
#  - winner[victor] and the loser[vanquished]
word <- scan(what="") # 검색단어(winner(o) -> victor(x)) 입력
scores = laply(sentences, function(sentence, posDic, negDic) {
sentence = gsub('[[:punct:]]', '', sentence) #문장부호 제거
sentence = gsub('[[:cntrl:]]', '', sentence) #특수문자 제거
sentence = gsub('\\d+', '', sentence) # 숫자 제거
sentence = tolower(sentence) # 모두 소문자로 변경(단어가 모두 소문자 임)
word.list = str_split(sentence, '\\s+') # 공백 기준으로 단어 생성 -> \\s+ : 공백 정규식, +(1개 이상)
words = unlist(word.list) # unlist() : list를 vector 객체로 구조변경
pos.matches = match(words, posDic) # words의 단어를 posDic에서 matching
neg.matches = match(words, negDic)
pos.matches = !is.na(pos.matches) # NA 제거, 위치(숫자)만 추출
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches) # 긍정 - 부정
return(score)
}, posDic, negDic)
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
sentimental = function(sentences, posDic, negDic){
scores = laply(sentences, function(sentence, posDic, negDic) {
sentence = gsub('[[:punct:]]', '', sentence) #문장부호 제거
sentence = gsub('[[:cntrl:]]', '', sentence) #특수문자 제거
sentence = gsub('\\d+', '', sentence) # 숫자 제거
sentence = tolower(sentence) # 모두 소문자로 변경(단어가 모두 소문자 임)
word.list = str_split(sentence, '\\s+') # 공백 기준으로 단어 생성 -> \\s+ : 공백 정규식, +(1개 이상)
words = unlist(word.list) # unlist() : list를 vector 객체로 구조변경
pos.matches = match(words, posDic) # words의 단어를 posDic에서 matching
neg.matches = match(words, negDic)
pos.matches = !is.na(pos.matches) # NA 제거, 위치(숫자)만 추출
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches) # 긍정 - 부정
return(score)
}, posDic, negDic)
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
sentimental("I am victor and happy", posDic.final, negDic.final) # score = 1
sentimental("I am vanquished unHappy", posDic.final, negDic.final) # score = -1
match(c("a","b","c"), c("b","c","d")) #  NA  1  2
result<-sentimental(data[,2], posDic.final, negDic.final)
result
result<-sentimental(data[,2], posDic.final, negDic.final)
result
names(result) # "score" "text"
dim(result) # 100   2
data<-read.csv(file.choose()) # file.choose() 파일 선택
head(data,2)
dim(data) # 100   2
str(data) # 변수명 : company, review
setwd("C:/Rwork/Part-II")
posDic <- readLines("posDic.txt")
negDic <- readLines("negDic.txt")
length(posDic) # 2006
length(negDic) # 4783
library(plyr) # laply()함수 제공
library(stringr) # str_split()함수 제공
sentimental = function(sentences, posDic, negDic){
scores = laply(sentences, function(sentence, posDic, negDic) {
sentence = gsub('[[:punct:]]', '', sentence) #문장부호 제거
sentence = gsub('[[:cntrl:]]', '', sentence) #특수문자 제거
sentence = gsub('\\d+', '', sentence) # 숫자 제거
sentence = tolower(sentence) # 모두 소문자로 변경(단어가 모두 소문자 임)
word.list = str_split(sentence, '\\s+') # 공백 기준으로 단어 생성 -> \\s+ : 공백 정규식, +(1개 이상)
words = unlist(word.list) # unlist() : list를 vector 객체로 구조변경
pos.matches = match(words, posDic) # words의 단어를 posDic에서 matching
neg.matches = match(words, negDic)
pos.matches = !is.na(pos.matches) # NA 제거, 위치(숫자)만 추출
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches) # 긍정 - 부정
return(score)
}, posDic, negDic)
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
sentimental("I am victor and happy", posDic.final, negDic.final) # score = 1
sentimental("I am vanquished unHappy", posDic.final, negDic.final) # score = -1
result<-sentimental(data[,2], posDic.final, negDic.final)
result
names(result) # "score" "text"
dim(result) # 100   2
result$text
result$score # 100 줄 단위로 긍정어/부정어 사전을 적용한 점수 합계
# score값을 대상으로 color 칼럼 추가
result$color[result$score >=1] <- "blue"
result$color[result$score ==0] <- "green"
result$color[result$score < 0] <- "red"
# 감성분석 결과 차트보기
plot(result$score, col=result$color) # 산포도 색생 적용
barplot(result$score, col=result$color, main ="감성분석 결과화면") # 막대차트
# 5) 단어의 긍정/부정 분석
# 감성분석 빈도수
table(result$color)
#blue green   red
#53    11    36
# score 칼럼 리코딩
result$remark[result$score >=1] <- "긍정"
result$remark[result$score ==0] <- "중립"
result$remark[result$score < 0] <- "부정"
sentiment_result<- table(result$remark)
# 제목, 색상, 원크기
pie(sentiment_result, main="감성분석 결과",
col=c("blue","red","green"), radius=0.8) # ->  1.2
dim(data) # 100   2
str(data) # 변수명 : company, review
sentimental("I am victor and happy", posDic.final, negDic.final) # score = 1
#1     2 I am victor and happy
sentimental("I am vanquished unHappy", posDic.final, negDic.final) # score = -1
# 부정단어 테스트
match(c("a","b","c"), c("b","c","d")) #  NA  1  2
plot(result$score, col=result$color) # 산포도 색생 적용
barplot(result$score, col=result$color, main ="감성분석 결과화면") # 막대차트
table(result$color)
result$remark[result$score >=1] <- "긍정"
result$remark[result$score ==0] <- "중립"
result$remark[result$score < 0] <- "부정"
sentiment_result<- table(result$remark)
sentiment_result<
sentiment_result
result$remark[result$score >=1] <- "긍정"
result$remark[result$score ==0] <- "중립"
result$remark[result$score < 0] <- "부정"
sentiment_result<- table(result$remark)
sentiment_result
sentiment_result<- table(result$remark)
sentiment_result
# 제목, 색상, 원크기
pie(sentiment_result, main="감성분석 결과",
col=c("blue","red","green"), radius=0.8) # ->  1.2
